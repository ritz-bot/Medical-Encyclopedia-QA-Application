section 9.105

### Project Overview
The project is a Medical RAG (Retrieval-Augmented Generation) Chatbot. The speaker assumes the viewer has seen the demo video and knows the basic concept. It's built using a tech stack that's not basic but key components include Hugging Face, Jenkins, Aqua Trivy, Flask, and AWS Cloud. Jenkins and Aqua Trivy are highlighted as MVPs (Most Valuable Products/Tools) used in over 5000 companies and very famous.

### Tech Stack in Detail
- **Hugging Face**: Used for both the LLM (Large Language Model) and the embedding model. 
  - LLM: Mistral AI, an open-source model developed by a French startup. It's free; create an account, generate an access token, and use it to access the model. The LLM generates output based on input.
  - Embedding Model: Example is all-MiniLM-L6-v4 (or similar, referred to as mini L4). Converts documents into embeddings for storage in the vector store.
  - Data Source: A PDF file (medical encyclopedia). Extract content, convert to documents, then to embeddings using the embedding model.

- **Vector Store: FAISS**: A local vector store to store embeddings. Full form: Facebook AI Similarity Search. Developed by Meta (parent company of Facebook, Instagram; Facebook was renamed Meta in the past 2-3 years). Documents can't be stored directly; they must be converted to embeddings first.

- **LangChain**: A generative AI framework to interact with the models (LLM and embeddings).

- **PyPDF**: Library to read the contents of the PDF file, extract text, and convert it into documents. These documents are then turned into embeddings and stored in the vector store.

- **Flask and HTML/CSS**:
  - Flask: Handles backend operations. Processes user inputs (e.g., queries), performs logic, and returns responses.
  - HTML/CSS: Creates the UI (frontend). Users input data via forms (e.g., age in an example). Frontend sends requests to backend; backend processes and returns results to frontend for display.
  - Example: User enters age 18 in frontend form. Backend (Flask) checks if >=18; returns "eligible" or "not eligible" for driving, displayed on frontend. If age 16 or 15, shows "not eligible".

- **Docker**: Used for containerization during deployment. Create a Dockerfile to dockerize the application, allowing it to run as a container on AWS.

- **Aqua Trivy (referred to as Trivy or TV)**: An open-source security scanner. Scans Docker images for vulnerabilities (weaknesses or flaws in the application/Docker container that hackers can exploit for unauthorized access, data leakage, or misuse).
  - How it works: Detects issues like outdated libraries with security bugs (e.g., Flask version 2.0.1 has bugs; suggests upgrading to 2.1.0 where bugs are fixed). Scans requirements.txt, gives warnings and suggestions to preserve security from hackers.

- **Jenkins**: Used for CI/CD pipelines (Continuous Integration/Continuous Deployment). Automates: Code checkout from GitHub, building Docker image, pushing image, scanning with Trivy, and deploying to AWS Runner.
  - AWS Runner: A service by AWS to easily run applications on the cloud. Connect it to Jenkins; click "build" button to execute the entire pipeline.

- **GitHub**: Acts as SCM (Source Code Management). Stores all project code, tracks changes, and maintains records.

- **AWS Cloud**: For deployment. Includes ECR (Elastic Container Registry) to store Docker images (similar to Docker Hub or Google Container Registry). Images are pushed to ECR, then deployed to AWS Runner.

### Project Workflow
The workflow is detailed and sequential:

1. **Project and API Setup**: Create a virtual environment. Set up logging and custom exceptions (needed for production-grade projects). Define project structure with various folders and files. Create setup file, requirements file, and environment file to store API keys (e.g., Hugging Face access token).

2. **Hugging Face Setup**: Create/sign in to Hugging Face account, generate access token, store in environment variables. In configuration code, access environment variables, specify LLM version (Mistral AI) and embedding model.

3. **PDF Loader Code**: PDF stored in a data folder. Fetch PDF, read contents using PyPDF, convert to documents, then to chunks using LangChain.

4. **Embeddings Code**: Convert chunks into embeddings using the Hugging Face embedding model.

5. **Vector Store Code**: Store embeddings in FAISS vector store.

6. **Data Loader Code**: Combines the above: PDF loader + embeddings + vector store into one pipeline. Reads PDF, converts to chunks, to embeddings, and stores in vector store.

7. **LLM Setup Video**: Connect Hugging Face API to the specific LLM (Mistral AI). Define parameters: Prompt format, output format, temperature (0-1 scale for creativity; 0 = non-creative, 1 = highly creative; suggest 0.3-0.7 for balanced results. High temperature gives weird results; low gives minimal/no results).

8. **Retrieval Code**: For user queries, retrieve relevant data from the vector store (documents/embeddings). Send retrieved context to LLM for refined output. Creates a retriever to fetch and return answers.

9. **Main Application Code**: Combines frontend (HTML/CSS for UI/forms) and backend (Flask for logic/processing). As seen in demo video; handles user inputs and displays responses.

10. **Code Building Using GitHub**: Push all code to GitHub repository for storage and tracking.

11. **Dockerfile Creation**: Configures container creation: Install requirements, run setup files, specify Python image, define Flask port (5000), how to run the app (e.g., python app.py).

12. **Jenkins CI/CD Pipeline**:
    - Set up Jenkins on PC.
    - Integrate GitHub with Jenkins (code is in GitHub).
    - Pipeline steps: Checkout code from GitHub, build Docker image using Dockerfile, scan image with Trivy for vulnerabilities, push image to AWS ECR, deploy image to AWS Runner as an application.
    - All executes on clicking the "build" button in Jenkins.

The speaker hopes the viewer understands the main points, enjoys the project, learns something new, and can move forward.

9.106

### Project Setup for Medical RAG Chatbot

#### 1. Create Project Folder
- Navigate to your desired project directory.
- Create a new folder for the project.
- Name it "Medical rag chatbot" (or any preferred name, e.g., "rag medical chat bot").

#### 2. Open Folder in VS Code
- Open the folder in your terminal.
- Run the command `code .` to open the folder in VS Code.
- This will launch VS Code with the project folder as the workspace.

#### 3. Create and Activate Virtual Environment
- In VS Code, go to the top bar and select Terminal > New Terminal.
- Ensure the terminal is opened as Command Prompt.
- Run the command: `python -m venv venv` (you can use any name for the environment, but "venv" is used here).
- Wait 10-15 seconds for the virtual environment folder to be created (it will appear on the left side in VS Code).
- Activate the environment: Run `venv\Scripts\activate`.
- Confirmation: The terminal prompt will show `(venv)` at the beginning, indicating successful creation and activation.

#### 4. Create requirements.txt File
- In the root directory, create a file named `requirements.txt`.
- List the following libraries (one per line) for the project:
  - langchain (for core functionality).
  - langchain_community (additional community features).
  - langchain_huggingface (for Hugging Face integration).
  - faiss-cpu (for vector stores, used on CPU).
  - pypdf (for PDF parsing, to be used later).
  - huggingface_hub (for using Hugging Face API).
  - flask (for creating the web app).
  - python-dotenv (for handling environment variables, e.g., storing API keys).

#### 5. Set Up Project Structure
- In the root directory, create a folder named `app`.
- Inside `app`, create an `__init__.py` file (to treat `app` as a Python package).
- Inside `app`, create the following subfolders:
  - `common`: For common files like logging and custom exceptions.
    - Inside `common`, create `__init__.py` (to treat it as a package).
    - Create `logger.py` (for logging functionality).
    - Create `custom_exception.py` (for custom exceptions).
    - Copy the code for `logger.py` and `custom_exception.py` from GitHub (provided in resources or previous sections; the code has been explained thoroughly elsewhere).
  - `components`: For app components like LLM, retriever, PDF loader, data loader files, etc.
  - `config`: For configurations, such as loading APIs and models.
  - `templates`: For HTML web pages (required for Flask).

#### 6. Create setup.py File
- In the root directory, create a file named `setup.py`.
- Copy the code for `setup.py` from GitHub or previous resources (explained in detail elsewhere).
- Purpose: 
  - Collects and installs all packages listed in `requirements.txt`.
  - Handles project package management (e.g., treats folders with `__init__.py` as packages).
- Modify the project name in the code to "Rag medical Chatbot" (or your preferred name).
- Save the file.

#### 7. Install Requirements and Set Up Package
- Ensure the virtual environment is activated.
- In the terminal, run: `pip install -e .`.
- This triggers `setup.py`, installs all libraries from `requirements.txt`, and sets up the project as a package.
- Wait for installation to complete (time depends on internet speed and number of libraries).

#### 8. Prepare Data Folder and Download PDF
- In the root directory, create a folder named `data`.
- For the medical chatbot, download a medical PDF (e.g., an "encyclopedia of medicine" PDF, around 10 MB in size, from GitHub resources).
- You can use any relevant medical PDF.
- Copy the downloaded PDF from your downloads folder into the `data` directory.

#### 9. Set Up Hugging Face Account and API Token
- Open any browser and search for "Hugging Face".
- Create an account if you don't have one.
- Go to your profile section.
- Navigate to "Access Tokens".
- Generate a new token:
  - Select the "Write" section (options: fine-grained, read, write).
  - Name the token (any name).
  - Create the token; it will provide an access token key.
- Copy the token (create your own; do not use others as they may be deactivated).

#### 10. Create .env File for Environment Variables
- In the root directory, create a file named `.env`.
- Add the following lines:
  - `HF_TOKEN=your_huggingface_token` (paste the copied token here).
  - `HUGGINGFACE_API_TOKEN=your_huggingface_token` (paste the same token here; both are used for Hugging Face API access).
- Save the file.
- Note: This stores API keys securely for use via python-dotenv.

#### 11. Next Steps
- Wait for all library installations to complete successfully.
- Once done, proceed to the next video for further project development (e.g., PDF parsing, components implementation).

9.107

### Configuration Code Setup for Medical RAG Chatbot

#### 1. Update .env File
- Navigate to the `.env` file in the root directory.
- Change the entry from "HUGGINGFACE_API_TOKEN" to "HUGGINGFACE_HUB_API_TOKEN" (ensure the key is updated to match this exact name).
- Save the file.
- You can close the file after saving.

#### 2. Set Up config Directory as a Package
- Go to the `app` directory.
- Inside `app`, navigate to the `config` directory.
- Create a file named `__init__.py` inside `config` to treat it as a Python package.
- After creating `__init__.py`, open the terminal (ensure virtual environment is activated).
- Run the command: `pip install -e .` to update the packages, as a new `__init__.py` file has been added.
- Note: Every time you create a new `__init__.py` file or add a new library to the project, run `pip install -e .` to keep packages up to date.

#### 3. Create config.py File
- Inside the `config` directory, create a new file named `config.py`.
- This file will contain all project configurations.

#### 4. Write Configurations in config.py
- Import the `os` library at the top: `import os`.
- Fetch the Hugging Face token from the `.env` file:
  - `HF_TOKEN = os.environ.get('HF_TOKEN')` (use the exact name specified in your `.env` file, e.g., 'HF_TOKEN').
  - This stores the token in the `HF_TOKEN` variable.
- Define additional configuration variables:
  - Hugging Face repo ID (for the LLM model).
  - DB FAISS path (path for the vector store, to be created later).
  - Data path (path to input data, i.e., the PDF).
  - Chunk size: Set to 500 (related to LLM text processing).
  - Chunk overlap: Set to 50 (related to LLM text processing).
- Set the data path:
  - `DATA_PATH = 'data/'` (points to the `data` directory in the root where the PDF is stored; any files in `data` will be fetched as input).
- Set the DB FAISS path:
  - `DB_FAISS_PATH = 'vectorstore/db_faiss'` (this will create a `vectorstore` folder in the root directory, with a `db_faiss` subfolder inside to store all vector store data for the LLM).
- Set the Hugging Face repo ID:
  - Go to the Hugging Face website (huggingface.co).
  - Search for "models".
  - Search specifically for "Mistral".
  - Select "Mistral AI" and navigate to "Mistral-7B-Instruct-v0.3".
  - Copy the model path: `'mistralai/Mistral-7B-Instruct-v0.3'`.
  - Paste it as: `HUGGINGFACE_REPO_ID = 'mistralai/Mistral-7B-Instruct-v0.3'`.
  - Notes on the model (as per the video): It is free to use, open source, and Hugging Face provides about 100 requests per day, which is sufficient for testing purposes.
    - **Updated Note (as of August 2025)**: The model is an instruct fine-tuned version of Mistral-7B-v0.3 with extended vocabulary (32,768), v3 tokenizer support, and function calling. It lacks moderation mechanisms. It is open source (though license details like Apache 2.0 are typically applicable but not explicitly restated). Inference API is not directly deployed by providers, but accessible via Hugging Face tools; access requires login and sharing contact info. Free user rate limits for Hugging Face Inference API have changed: They are strict, often resetting hourly rather than daily, with recent reductions (e.g., from higher daily calls to more limited or paid tiers starting at ~$0.10 for certain usages; Pro plans offer higher limits like 20,000 requests/day but are not unlimited).

#### 5. Completion and Next Steps
- This completes all configurations in `config.py`.
- Proceed to the next video for further development.

9.108

### PDF Loader Code Setup for Medical RAG Chatbot

#### 1. Set Up components Directory as a Package
- Navigate to the `app` directory.
- Inside `app`, go to the `components` directory.
- Create a file named `__init__.py` inside `components` to treat it as a Python package.
- After creating `__init__.py`, run the command `pip install -e .` in the terminal (ensure virtual environment is activated) to update and treat the `components` directory as a package.
- Note: The installation may take time; once successful, you can close the terminal.

#### 2. Create pdf_loader.py File
- Inside the `components` directory, create a new file named `pdf_loader.py`.
- Purpose: This file handles loading PDF files from the data directory and splitting their content into text chunks for the RAG chatbot.
- Code can be copied from GitHub (mentioned in the resource section) if you don't want to write it from scratch, but for learning, follow the steps below.

#### 3. Imports in pdf_loader.py
- Import the following libraries and modules:
  - `import os` (for path handling).
  - From `langchain_community.document_loaders`:
    - `DirectoryLoader` (to load files from a directory).
    - `PyPDFLoader` (specifically for loading PDF files).
  - From `langchain.text_splitter`:
    - `RecursiveCharacterTextSplitter` (to split text into chunks; e.g., splits sentences like "My name is Dan" into smaller parts like "My", "name", "is", "Dan").
  - From `app.common.logger`:
    - `get_logger` (for logging functionality).
  - From `app.common.custom_exception`:
    - `CustomException` (for custom error handling).
  - From `app.config.config`:
    - `DATA_PATH` (path to the data directory containing PDFs).
    - `CHUNK_SIZE` (chunk size for text splitting, defined as 500 in config).
    - `CHUNK_OVERLAP` (chunk overlap for text splitting, defined as 50 in config).

#### 4. Initialize Logger
- Add: `logger = get_logger(__name__)` 
- This initializes the logger successfully using the module's name.

#### 5. Function: load_pdf_files()
- Purpose: Loads all PDF files from the `data` directory (handles any number of PDFs: 1, 3, 15, etc.; if multiple PDFs are added to `data`, it fetches all of them).
- Define the function: `def load_pdf_files():`
- Use a try-except block for error handling.
- Inside try:
  - Check if the data path exists: `if not os.path.exists(DATA_PATH):`
    - If not, raise `CustomException("Data path does not exist")`.
  - If it exists: `logger.info(f"Loading files from {DATA_PATH}")`.
  - Initialize the loader: `loader = DirectoryLoader(DATA_PATH, glob="*.pdf", loader_cls=PyPDFLoader)`
    - `DATA_PATH`: The directory to load from.
    - `glob="*.pdf"`: Specifies to fetch only PDF files (ignores text files, PowerPoint, etc.).
    - `loader_cls=PyPDFLoader`: Uses PyPDFLoader for PDF handling.
  - Load the documents: `documents = loader.load()`
    - All loaded PDF content is stored in `documents`.
  - Check if documents were loaded:
    - `if not documents: logger.warning("No PDFs were found")` (warning if no PDFs in data folder; suggests adding data).
    - Else: `logger.info(f"Successfully fetched {len(documents)} documents")` (logs the number of documents fetched, e.g., 1 PDF shows "1 document", 5 PDFs shows "5 documents").
  - Return the documents: `return documents`.
- Inside except: `except Exception as e:`
  - Create error message: `error_message = CustomException("Failed to load PDFs")`.
  - Raise the custom exception: `raise error_message`.
  - Log the error: `logger.error(str(error_message))`.
  - Optionally return an empty list: `return []` (this is optional and can be removed without affecting functionality; used if loading fails).

#### 6. Function: create_text_chunks(documents)
- Purpose: Takes the loaded documents, extracts their content, and splits it into smaller text chunks (e.g., breaking down text for LLM processing).
- Define the function: `def create_text_chunks(documents):`
- Use a try-except block for error handling.
- Inside try:
  - Check if documents exist: `if not documents:`
    - Raise `CustomException("No documents were found")`.
  - Log the splitting process: `logger.info(f"Splitting {len(documents)} documents into chunks")` (e.g., logs "Splitting 1 documents" or "Splitting 2 documents").
  - Initialize the text splitter: `text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)`
    - Uses the predefined `CHUNK_SIZE` (500) and `CHUNK_OVERLAP` (50) from config.
  - Split the documents: `text_chunks = text_splitter.split_documents(documents)`
    - Splits all document content into chunks, stored in `text_chunks`.
  - Log the result: `logger.info(f"Generated {len(text_chunks)} text chunks")` (e.g., shows how many chunks were created).
  - Return the chunks: `return text_chunks`.
- Inside except: `except Exception as e:`
  - Similar to the previous function: `error_message = CustomException("Failed to generate chunks")`.
  - Raise `error_message`.
  - `logger.error(str(error_message))`.
  - Optionally return an empty list: `return []`.

#### 7. Summary of pdf_loader.py
- Imports: Langchain document loaders (DirectoryLoader, PyPDFLoader), text splitter (RecursiveCharacterTextSplitter), logger, custom exception, and configurations (DATA_PATH, CHUNK_SIZE, CHUNK_OVERLAP).
- Initialize logger.
- Function `load_pdf_files()`: Loads all PDFs from the data directory, checks existence, logs details, handles multiple files, and returns documents (with exception handling).
- Function `create_text_chunks(documents)`: Splits loaded document content into chunks using the text splitter, logs the process, and returns the chunks (with exception handling).
- Overall: The file fetches PDF data and prepares it as chunked text for further use in the chatbot.

#### 8. Next Steps
- Proceed to the next video for further development.


9.109

### Embeddings Code Setup for Medical RAG Chatbot

#### 1. Create embeddings.py File
- Navigate to the `app` directory.
- Inside `app`, go to the `components` directory.
- Create a new file named `embeddings.py`.

#### 2. Imports in embeddings.py
- Import from LangChain for Hugging Face embeddings: `from langchain_huggingface import HuggingFaceEmbeddings`.
- Import logger and custom exception:
  - `from app.common.logger import get_logger`
  - `from app.common.custom_exception import CustomException`

#### 3. Initialize Logger
- Add: `logger = get_logger(__name__)`
- This initializes the logger using the module's name.

#### 4. Function: get_embedding_model()
- Purpose: Loads an embedding model to convert text chunks (from the previous PDF loader step) into embeddings. Models understand embeddings (numerical representations), not raw text chunks, so this step is necessary for the RAG process.
- Define the function: `def get_embedding_model():`
- Use a try-except block for error handling.
- Inside try:
  - Log the initialization: `logger.info("Initializing our Hugging Face embedding model")`.
  - Load the model: `model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")`
    - Notes: This is the most common and widely used embedding model (Sentence Transformers' all-MiniLM-L6-v2). You can choose other models by changing the name.
    - To select alternatives: Go to Hugging Face, search for embeddings (e.g., "sentence transformer").
    - For domain-specific: Search for "medical embedding" on Hugging Face; there are models trained specifically on medical data (better for medical chatbots), law data, etc.
  - Log successful loading: `logger.info("Hugging Face embedding model loaded successfully")`.
  - Return the model: `return model`.
- Inside except: `except Exception as e:`
  - Create error message: `error_message = CustomException("Error occurred while loading embedding model")`.
  - Raise the custom exception: `raise error_message`.
  - Log the error: `logger.error(str(error_message))`.
  - Optionally raise or return; the code raises the error message.

#### 5. Summary of embeddings.py
- Imports: HuggingFaceEmbeddings from LangChain, logger, and custom exception.
- Initialize logger.
- Function `get_embedding_model()`: Loads the embedding model (specifically "sentence-transformers/all-MiniLM-L6-v2"), logs the process, handles exceptions, and returns the model for converting text chunks to embeddings.

#### 6. Updated Notes (as of August 2025)
- The "sentence-transformers/all-MiniLM-L6-v2" model maps sentences and paragraphs to a 384-dimensional vector space, suitable for tasks like clustering, semantic search, and RAG. It remains one of the most downloaded and reliable open-source embedding models on Hugging Face, with variants like ONNX and GGUF for optimized inference.
- For medical data: Better options include domain-specific models like PubMedBERT (strong for medical literature) or ClinicalBERT (trained on a 1.2B-word multicenter medical corpus). Recent benchmarks (e.g., MTEB leaderboard) recommend models like all-mpnet-base-v2 for general RAG, but for medical precision, fine-tuned models like those from medicalai or gte-base perform well. Hugging Face's text-embeddings-inference library is recommended for fast deployment in 2025.

#### 7. Next Steps
- This completes the embeddings code.
- Proceed to the next video for further development.

9.110

### Vector Store Code Setup for Medical RAG Chatbot

#### 1. Create vector_store.py File
- Navigate to the `app` directory.
- Inside `app`, go to the `components` directory.
- Create a new file named `vector_store.py`.

#### 2. Imports in vector_store.py
- Import the vector store (FAISS): `from langchain_community.vector_stores import FAISS` (this is the vector store being used).
- Import the embedding function from previous file: `from app.components.embeddings import get_embedding_model` (to get the embedding model).
- Import logger and custom exception:
  - `from app.common.logger import get_logger`
  - `from app.common.custom_exception import CustomException`
- Import configuration: `from app.config.config import DB_FAISS_PATH` (the path where the vector store will be stored).
- Also import `os` (mentioned later in the code for path checking): `import os`.

#### 3. Initialize Logger
- Add: `logger = get_logger(__name__)`
- This initializes the logger using the module's name (after imports).

#### 4. Overview of Functions
- Two functions will be created:
  - One to load an existing (previous) vector store (to avoid recreating it every time the app runs).
  - One to create and save a new vector store (for the first time or when needed).
- Workflow: If creating for the first time, use the create function. If it already exists, use the load function to reuse it.

#### 5. Function: load_vector_store()
- Purpose: Loads an existing vector store if it exists at the specified path.
- Define the function: `def load_vector_store():`
- Use a try-except block for error handling.
- Inside try:
  - Load the embedding model: `embedding_model = get_embedding_model()` (calls the function from embeddings.py; loads the all-MiniLM-L6-v2 model, stored in `embedding_model`).
  - Check if the vector store exists: `if os.path.exists(DB_FAISS_PATH):`
    - If yes: `logger.info("Loading existing vector store")`.
    - Load and return it: `return FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)`
      - Loads from `DB_FAISS_PATH`.
      - Passes the `embedding_model`.
      - Sets `allow_dangerous_deserialization=True` (required for loading).
  - Else (if it doesn't exist): `logger.warning("No vector store found")`.
- Inside except: `except Exception as e:`
  - Create error message: `error_message = CustomException("Failed to load vector store")`.
  - Log the error: `logger.error(str(error_message))`.
  - Note: The transcript implies raising or handling the exception, but doesn't explicitly say to raise it here; focus is on logging.

#### 6. Function: save_vector_store(text_chunks)
- Purpose: Creates a new vector store from text chunks (converts chunks to embeddings and saves).
- Define the function: `def save_vector_store(text_chunks):` (passes `text_chunks` as parameter; these are the split text from PDF loader).
- Note: It's a function, not a method (no class used).
- Use a try-except block for error handling.
- Inside try:
  - Check if text chunks are provided: `if not text_chunks:`
    - Raise `CustomException("No chunks were found")`.
  - If chunks exist: `logger.info("Generating your new vector store")`.
  - Load the embedding model: `embedding_model = get_embedding_model()` (same as in load function).
  - Create the vector store: `db = FAISS.from_documents(text_chunks, embedding_model)`
    - Uses `FAISS.from_documents` to create from PDF documents' text chunks.
    - Requires two parameters: `text_chunks` and `embedding_model`.
  - Log saving: `logger.info("Saving vector store")`.
  - Save it: `db.save_local(DB_FAISS_PATH)`
    - Saves to the path `DB_FAISS_PATH`.
  - Log success: `logger.info("Vector store saved successfully")`.
  - Return the db: `return db`.
- Inside except: `except Exception as e:`
  - Similar to load function: `error_message = CustomException("Failed to create new vector store")`.
  - Log the error: `logger.error(str(error_message))`.
  - Note: Copy the exception handling from the load function and adjust the message.

#### 7. Summary of vector_store.py
- Imports: FAISS from langchain_community, get_embedding_model from components.embeddings, logger, custom exception, DB_FAISS_PATH from config.config, and os.
- Initialize logger.
- Function `load_vector_store()`: Loads existing FAISS vector store if it exists using the embedding model, with logging and exception handling (warns if not found).
- Function `save_vector_store(text_chunks)`: Creates a new FAISS vector store from text chunks using the embedding model, saves it locally, with checks, logging, and exception handling.
- Overall: Handles creation and loading of the vector store to persist it across app runs (avoids recreating every time).

#### 8. Next Steps
- This completes the code for creating and loading the vector store.
- In the next video, combine the PDF loader, embeddings, and vector store files into a new file to actually create/load the vector store in practice.
- Proceed to the next video for further development.

9.111

### Summary of the Transcript: Creating the Data Loader in the Project

Bhai, yeh transcript ek video ka hai jisme data_loader.py file banate hain, jo project ke components directory mein important file hai. Yeh file embeddings.py, pdf_loader.py, aur vector_store.py ko use karke PDFs ko process karta hai aur vector store create karta hai (Faiss-based, lagta hai). Main details miss nahi karunga – step-by-step code aur explanation cover karunga. Yeh likely previous AI agent project ka continuation hai, jahaan PDFs se data load karke vector store banate hain for retrieval or search.

#### Key Concept: Data Loader Ka Role
- Data loader basically PDFs ko load karta hai, unko text chunks mein convert karta hai, aur phir embeddings use karke vector store mein save karta hai.
- Yeh components directory mein most important file hai, kyunki yeh saare previous files (embeddings, pdf_loader, vector_store) ko ek saath use karta hai.
- Vector store create karne ke liye yeh step-by-step kaam karta hai: Load PDFs → Create chunks → Save to vector store.

#### Code Structure in data_loader.py
File banane ke steps:

1. **Imports**:
   - `import os` (basic library).
   - From `app.components.pdf_loader`: `load_pdf_files` aur `create_text_chunks` (PDFs load aur chunk karne ke functions, previous video mein banaye gaye).
   - From `app.components.vector_store`: `save_vector_store` (vector store save karne ka function, jo embeddings.py pe depend karta hai).
   - From `app.config.config`: `DB_FAISS_PATH` (vector store ka path, Faiss DB ke liye).
   - Logger aur exception: `from app.common.logger import get_logger` aur `from app.common.custom_exception import CustomException`.
   - Logger initialize: `logger = get_logger(__name__)` (magic method `__name__` use karke).

   Note: Sirf yeh imports needed hain, baaki extra nahi.

2. **Function Definition: process_and_store_pdfs()**
   - Yeh main function hai, jo empty start hoti hai phir code add karte hain.
   - Pehle logging: `logger.info("Making the vector store")`.
   - Step 1: PDFs load karo from 'data' directory (project ke outside ya app ke andar, transcript mein adjust kiya gaya).
     - `documents = load_pdf_files()` (yeh function pdf_loader se aata hai, aur isme already logging built-in hai, isliye extra log nahi add kiya).
   - Step 2: Documents ko text chunks mein convert karo.
     - `text_chunks = create_text_chunks(documents)` (pdf_loader se).
   - Step 3: Chunks ko vector store mein save karo.
     - `save_vector_store(text_chunks)` (vector_store.py se, jo embeddings load karta hai indirectly).
   - Success logging: `logger.info("Vector store created successfully")`.
   - Exception Handling:
     - `except Exception as e:` block.
     - Error message create: `error_message = CustomException("Failed to create vector store", e)`.
     - Log error: `logger.error(str(error_message))`.

   Yeh function sirf itna hi karta hai – teen steps: load, chunk, save. Vector store embeddings.py pe depend karta hai (indirectly via save_vector_store).

3. **Running the File**:
   - `__if __name__ == "__main__":` block add karo.
     - Yeh check karta hai ki file directly run ho rahi hai (e.g., terminal se).
     - Andar: `process_and_store_pdfs()` call karo.
   - Explanation: Agar koi terminal mein `python app/components/data_loader.py` run kare, toh yeh block execute hoga. Yeh ensure karta hai ki function sirf tab chale jab file standalone run ho, import hone pe nahi.

#### Execution Demo
- Terminal kholo (new terminal or command prompt).
- Command: `python app/components/data_loader.py` (app directory ke andar components folder mein file hai).
- Kaam: `__name__ == "__main__"` true hone pe function chalega.
  - PDFs 'data' folder se load honge.
  - Chunks banenge.
  - Embeddings create honge aur vector store save hoga (DB_FAISS_PATH pe).
- Time lagega kyunki PDFs load, chunking, embeddings computation heavy hai.
- Logs show honge:
  - "Making the vector store" start pe.
  - Load_pdf_files ke built-in logs.
  - "Vector store created successfully" end pe.
- Output: Left pane mein 'vector_store' folder banega, andar 'db_faiss' subfolder mein files like 'index.faiss' aur 'index.pkl' (transcript mein 'index dot files' aur 'index dot SQL' bola, but Faiss ke liye typically .faiss aur .pkl hote hain – shayad typo, SQL nahi Faiss hai).
- Success: Vector store properly created, logs generate hue.

#### Additional Notes from Transcript
- 'data' folder initially app ke andar tha, but outside move kiya for better structure.
- Pdf_loader mein logging already hai, isliye extra nahi add kiya load step pe.
- Vector store vector_store.py pe depend karta hai, jo embeddings load karta hai.
- Next video pe move karne ka mention end mein.
- No errors in demo – sab smooth chala, vector store files ban gaye.

Bhai, yeh full detailed summary hai – code snippets, steps, aur explanations miss nahi kiye. Agar yeh project resume ke liye hai (jaise previous chat mein), toh yeh part RAG (Retrieval-Augmented Generation) system ke vector store creation ko highlight karta hai, jo AI/ML roles mein valuable hai. Koi extra doubt ho toh bata!

9.112

Updated Summary of the Transcript: Writing Code for LLM Setup in llm.py
Bhai, yeh transcript ek video ka hai jisme Large Language Model (LLM) setup ke liye code likhte hain – file llm.py (transcript mein "lm dot pi" bola, but clearly llm.py hai) components directory mein. Yeh Hugging Face se Mistral-7B-Instruct-v0.3 model load karta hai using LangChain. Main details miss nahi karunga: step-by-step imports, function code, explanations, aur future use sab cover karunga. Transcript mein kuch typos hain (e.g., "Mr. Lai" Mistral AI, "HDF token" HF_TOKEN), but intent clear hai – medical chatbot project ka continuation, vector store ke saath integrate hone waala.
Video Introduction

Video ka focus: LLM setup code likhna.
Directory: Components folder mein jaao aur new file create karo: llm.py.

Imports Section

Basic imports for LangChain LLMs:

from langchain.llms import HuggingFaceHub (Hugging Face Hub se model load karne ke liye).


Config se model details:

from app.config.config import HF_TOKEN, HUGGINGFACE_REPO_ID

HUGGINGFACE_REPO_ID: 'mistralai/Mistral-7B-Instruct-v0.3' (Mistral AI model, instruct version 0.3 – yeh woh repo hai jahaan se model load hoga).
HF_TOKEN: Hugging Face API token (.env file se aata hai).




Logger aur exception handling:

from app.common.logger import get_logger
from app.common.custom_exception import CustomException


Logger initialize: logger = get_logger(__name__) (magic method __name__ use karke, jo current module name deta hai).

Function Definition: load_llm

Function create karo: def load_llm(huggingface_repo_id: str = HUGGINGFACE_REPO_ID, hf_token: str = HF_TOKEN):

Parameters:

huggingface_repo_id: String, default se HUGGINGFACE_REPO_ID (repo jahaan model hai).
hf_token: String, default se HF_TOKEN (API token).


Yeh function two things need karta hai: Repo ID (model kahaan se load karna) aur token.


Try-Except Block:

Try mein:

Logging: logger.info("Loading LLM from Hugging Face") (ya kuch custom log, jaise video mein bola – "whatever log you want").
LLM initialize:

llm = HuggingFaceHub(

repo_id=repo_id, (parameter se aata hai, default Mistral repo).
model_kwargs={ (dictionary for model parameters):

'temperature': 0.3, (randomness control: Lower temperature = lower randomness, less creative output. Video mein explain: "We don't want much creative model, we want only less creative models." Higher temperature = more creative, but yahaan low rakha for precise responses).
'max_length': 256, (max tokens in response: Example diya – "What is your name?" ka response like "My name is Medical Bot" mein tokens count karta hai. Standard 256 rakha to limit response size).
'return_full_text': False (sirf last response return karo, nahi to full text including all intermediate steps. Reason: Time, money, memory save – huge responses avoid karo. Explain: Hugging Face free nahi truly, token limits exhaust ho jaate hain, charge lag sakta hai if too many tokens).


},
huggingfacehub_api_token=hf_token (token pass karo for authentication).


)
Basically, LLM load karne ke liye teen cheezein: Repo ID, model parameters (temperature, max_length, return_full_text), aur API token.


Success logging: logger.info("LLM loaded successfully")
Return: return llm (loaded model return karo).


Except block:

except Exception as e:
Error message: error_message = CustomException("Failed to load LLM", e) (custom exception with message aur original error).
Log error: logger.error(str(error_message))




Code simplicity: Video mein bola "I think it was simple enough." – Bas imports, logging, exception, aur do cheezein (repo ID aur token) se LLM load.

Additional Explanations

Temperature: Randomness measure – lower (0.3) = more deterministic, less creative (jaise medical queries ke liye suitable, nahi to wrong/creative answers aa sakte hain).
Max Length: Response ke tokens limit – e.g., short answers ke liye 256 enough.
Return Full Text False: Sirf final output, nahi to poora process text (time/money waste avoid).
Hugging Face Limits: Free but limited – too many tokens generate karo to limits exhaust, charge lag sakta hai.
Future Use: Yeh load_llm function next video mein use hoga for "retriever" – jo user queries ke answers fetch karega contextually from vector store.

Retriever: User question pe answer fetch, but context vector store ke data se (medical data in this case).
Example: "What is cancer?" → Medical details dega (disease info).
"Cure for cancer?" → No certain cure bataayega.
Irrelevant query (e.g., stocks) → No answer, kyunki context data ke according (medical PDFs/vector store se).
Important: Answers data-fed context mein hone chahiye, nahi to irrelevant.



End of Video

Next: Retriever video mein move, jahaan LLM use karke answers fetch karenge with vector store context.

9.113

Updated Summary of the Transcript: Writing Code for LLM Setup in llm.py
Bhai, yeh transcript ek video ka hai jisme Large Language Model (LLM) setup ke liye code likhte hain – file llm.py (transcript mein "lm dot pi" bola, but clearly llm.py hai) components directory mein. Yeh Hugging Face se Mistral-7B-Instruct-v0.3 model load karta hai using LangChain. Main details miss nahi karunga: step-by-step imports, function code, explanations, aur future use sab cover karunga. Transcript mein kuch typos hain (e.g., "Mr. Lai" Mistral AI, "HDF token" HF_TOKEN), but intent clear hai – medical chatbot project ka continuation, vector store ke saath integrate hone waala.
Video Introduction

Video ka focus: LLM setup code likhna.
Directory: Components folder mein jaao aur new file create karo: llm.py.

Imports Section

Basic imports for LangChain LLMs:

from langchain.llms import HuggingFaceHub (Hugging Face Hub se model load karne ke liye).


Config se model details:

from app.config.config import HF_TOKEN, HUGGINGFACE_REPO_ID

HUGGINGFACE_REPO_ID: 'mistralai/Mistral-7B-Instruct-v0.3' (Mistral AI model, instruct version 0.3 – yeh woh repo hai jahaan se model load hoga).
HF_TOKEN: Hugging Face API token (.env file se aata hai).




Logger aur exception handling:

from app.common.logger import get_logger
from app.common.custom_exception import CustomException


Logger initialize: logger = get_logger(__name__) (magic method __name__ use karke, jo current module name deta hai).

Function Definition: load_llm

Function create karo: def load_llm(huggingface_repo_id: str = HUGGINGFACE_REPO_ID, hf_token: str = HF_TOKEN):

Parameters:

huggingface_repo_id: String, default se HUGGINGFACE_REPO_ID (repo jahaan model hai).
hf_token: String, default se HF_TOKEN (API token).


Yeh function two things need karta hai: Repo ID (model kahaan se load karna) aur token.


Try-Except Block:

Try mein:

Logging: logger.info("Loading LLM from Hugging Face") (ya kuch custom log, jaise video mein bola – "whatever log you want").
LLM initialize:

llm = HuggingFaceHub(

repo_id=repo_id, (parameter se aata hai, default Mistral repo).
model_kwargs={ (dictionary for model parameters):

'temperature': 0.3, (randomness control: Lower temperature = lower randomness, less creative output. Video mein explain: "We don't want much creative model, we want only less creative models." Higher temperature = more creative, but yahaan low rakha for precise responses).
'max_length': 256, (max tokens in response: Example diya – "What is your name?" ka response like "My name is Medical Bot" mein tokens count karta hai. Standard 256 rakha to limit response size).
'return_full_text': False (sirf last response return karo, nahi to full text including all intermediate steps. Reason: Time, money, memory save – huge responses avoid karo. Explain: Hugging Face free nahi truly, token limits exhaust ho jaate hain, charge lag sakta hai if too many tokens).


},
huggingfacehub_api_token=hf_token (token pass karo for authentication).


)
Basically, LLM load karne ke liye teen cheezein: Repo ID, model parameters (temperature, max_length, return_full_text), aur API token.


Success logging: logger.info("LLM loaded successfully")
Return: return llm (loaded model return karo).


Except block:

except Exception as e:
Error message: error_message = CustomException("Failed to load LLM", e) (custom exception with message aur original error).
Log error: logger.error(str(error_message))




Code simplicity: Video mein bola "I think it was simple enough." – Bas imports, logging, exception, aur do cheezein (repo ID aur token) se LLM load.

Additional Explanations

Temperature: Randomness measure – lower (0.3) = more deterministic, less creative (jaise medical queries ke liye suitable, nahi to wrong/creative answers aa sakte hain).
Max Length: Response ke tokens limit – e.g., short answers ke liye 256 enough.
Return Full Text False: Sirf final output, nahi to poora process text (time/money waste avoid).
Hugging Face Limits: Free but limited – too many tokens generate karo to limits exhaust, charge lag sakta hai.
Future Use: Yeh load_llm function next video mein use hoga for "retriever" – jo user queries ke answers fetch karega contextually from vector store.

Retriever: User question pe answer fetch, but context vector store ke data se (medical data in this case).
Example: "What is cancer?" → Medical details dega (disease info).
"Cure for cancer?" → No certain cure bataayega.
Irrelevant query (e.g., stocks) → No answer, kyunki context data ke according (medical PDFs/vector store se).
Important: Answers data-fed context mein hone chahiye, nahi to irrelevant.



End of Video

Next: Retriever video mein move, jahaan LLM use karke answers fetch karenge with vector store context.


9.113

### Summary of the Transcript: Creating the Retrieval Chain in retrieval.py

Bhai, yeh transcript ek video ka hai jisme retrieval.py file banate hain components directory mein, jo RAG (Retrieval-Augmented Generation) system ke liye retrieval chain set up karta hai medical chatbot project mein. Yeh previous videos ka continuation hai – lm.py (LLM load), vector_store.py (vector store load/create), aur config se. Main details miss nahi karunga: step-by-step imports, code, functions, explanations, aur potential issues (jaise VS Code import error) sab cover karunga. Transcript mein typos hain (e.g., "retrieval dot pi" shayad retrieval.py, "lm" LLM, "HDF token" HF_TOKEN, "Lange Chain" LangChain), but intent clear hai – custom prompt aur QA chain banake answers fetch karna vector store se, LLM use karke.

#### Video Introduction
- Video ka focus: Retrieval code likhna (retriever means user queries ke answers fetch karna contextually from vector store).
- Directory: Components folder mein jaao aur new file create karo: retrieval.py.

#### Imports Section
- LangChain se retrieval QA:
  - `from langchain.chains import RetrievalQA` (transcript mein "retrieval qa" bola, with spelling issues like "r t r I I t r I r e v l e v qa" – yeh QA chain ke liye hai).
    - Note: VS Code mein import nahi show ho raha, but video mein bola "don't get confused, it is import" – shayad VS Code issue, LangChain ka nahi.
- Prompts ke liye:
  - `from langchain.prompts import PromptTemplate` (custom prompts banane ke liye).
- Previous components se:
  - `from app.components.lm import load_lm` (previous video se LLM load function).
  - `from app.components.vector_store import load_vector_store` (existing vector store load karne ke liye – vector_store.py mein do functions the: create new aur load existing; yahaan load use karenge).
- Config se:
  - `from app.config.config import HUGGINGFACE_REPO_ID, HF_TOKEN` (Hugging Face repo ID aur token – Mistral model ke liye).
- Logger aur exception:
  - `from app.common.logger import get_logger`
  - `from app.common.custom_exception import CustomException`
- OS library (but end mein remove kiya gaya, kyunki nahi chahiye tha):
  - `import os` (initially added, but later "remove this OS" bola).
- Logger initialize: `logger = get_logger(__name__)` (magic method `__name__` se).

Yeh sab imports aur logger init ke baad code start hota hai.

#### Custom Prompt Template
- Ek multi-line string define karo: `custom_prompt_template = """` (triple quotes use karke).
- Prompt content:
  - "Answer the following medical question in 2 to 3 lines maximum." (Short rakhne ka reason: Token limits exhaust na ho, kyunki long responses costly hote hain).
  - "Using only the information provided in the context." (Model ko restrict karo taaki out-of-context na jaaye – e.g., "what is cancer?" pe sirf cancer info, nahi lung/throat cancer mix).
  - Format: {context} (data from vector store), then {question} (user query), then Answer: (output).
- Explanation: Prompt medical-specific hai, context-bound (vector store ke data se), aur concise (2-3 lines max).

#### Function 1: set_custom_prompt()
- Def: `def set_custom_prompt():`
  - Return: `return PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])`
    - Template: Upar wala custom_prompt_template pass karo.
    - Input variables: ["context"] (data from vector store), ["question"] (user ka sawal).
- Purpose: Custom prompt ko LangChain ke PromptTemplate mein convert karo, jo QA chain mein use hoga.

#### Function 2: create_chain()
- Def: `def create_chain():` (empty start, phir try-except add).
- Try block:
  - Logging: `logger.info("Loading vector store for context")` (context fetch ke liye).
  - Load vector store: `db = load_vector_store()` (existing DB load, e.g., DB_FAISS_PATH se).
  - Load LLM: `llm = load_lm(huggingface_repo_id=HUGGINGFACE_REPO_ID, hf_token=HF_TOKEN)` (optional params, default se bhi chalega, but explicitly pass kiya for clarity).
  - Security checks (optional, for better code):
    - `if db is None: raise CustomException("Vector store not present or empty")`
    - `if llm is None: raise CustomException("LLM not loaded")`
  - Create QA chain: `qa_chain = RetrievalQA.from_chain_type(`
    - `llm=llm,` (loaded LLM pass).
    - `chain_type="stuff",` (chain type 'stuff' – simple stuffing of context into prompt).
    - `retriever=db.as_retriever(search_kwargs={"k": 1}),` (retriever from db; k=1 for most relevant document only).
      - Explanation of k: Default 1 rakha taaki sirf sabse relevant result (e.g., "what is cancer?" pe exact cancer info, nahi related like lung/breast cancer). Agar more context chahiye (randomness/bigger chain), to k=3 (standard) ya higher badhao.
    - `return_source_documents=False,` (source docs na return karo – unnecessary for chatbot, user ko extra info nahi chahiye).
    - `chain_type_kwargs={"prompt": set_custom_prompt()}` (custom prompt pass karo via set_custom_prompt).
  - `)` 
  - Success logging: `logger.info("Successfully created the QA chain")`
  - Return: `return qa_chain`
- Except block:
  - `except Exception as e:`
  - `error_message = CustomException("Failed to make a QA chain", e)`
  - `logger.error(str(error_message))`

- Overall: Chain create karne ke liye do main cheezein: Vector store (context ke liye) aur LLM (answers generate ke liye).

#### Additional Notes and Cleanup
- Retriever ka role: User answers fetch karna vector store ke context se – medical data pe based (e.g., cancer query pe relevant info, irrelevant like stocks pe no answer).
- OS import remove kiya end mein, kyunki nahi chahiye tha.
- VS Code issue: RetrievalQA import nahi show ho raha, but "it will work" bola – LangChain ka nahi, VS Code problem.
- Code confusion: "I know it's a bit confusing, but you can just copy paste if you want to save the time."

#### End of Video
- Next: Application code video mein move, jahaan retrieval chain use karke app banayenge aur test karenge (input/output generate hota hai ya nahi).
- Yeh retriever answers generate karega LLM se, custom prompt follow karke.

Bhai, yeh full detailed summary hai – code snippets, parameters explanations, logs, exceptions, aur workflow sab include kiye. Agar yeh medical chatbot ka part hai (jaise previous transcripts/logs se), toh yeh RAG pipeline complete karta hai: Vector store se retrieve → LLM se generate. Koi doubt ho toh bata!

9.114

### Summary of the Transcript: Building the Application Code for the Medical RAG Chatbot

Bhai, yeh transcript ek video ka hai (duration 34:57 / 35:01) jisme medical RAG chatbot ke liye main application code likhte hain – Flask use karke web app banate hain, jo user queries handle karta hai, LLM responses generate karta hai retriever chain se, aur chat history maintain karta hai session ke through. Video mein HTML template se shuruaat hoti hai, phir application.py file ka full code, demo, fixes (LangChain updates), aur explanation. Main details miss nahi karunga: step-by-step code, explanations, optional parts, aur demo issues sab cover karunga. Transcript mein kuch repetitions aur typos hain (e.g., "ginger" Jinja, "LM" LLM, "FX token" HF_TOKEN, "VR tag" BR tag, "long chain" LangChain, "app dot pi" application.py), but intent clear hai – basic Flask app with chatbot functionality.

#### HTML Template Setup (index.html)
- Directory: App folder mein 'templates' subfolder jaao aur new file create karo: index.html.
- Code Source: Scratch se nahi likh rahe, kyunki video web development pe nahi – ChatGPT se generate kiya gaya basic code use karo.
  - Options: 
    - Video wala code copy karo.
    - GitHub repo (course mein given) se templates/index.html copy karo.
    - Us code ko ChatGPT mein paste karo aur modify karwao for better look (e.g., more attractive UI, animations), but concept mat badlo (e.g., form with textarea name="prompt" for user input, send button for POST request).
- Purpose: Yeh frontend hai – user query enter karne ke liye form (textarea name="prompt"), send button, aur responses display karne ke liye (Jinja2 templates use karke {{ messages }} loop se chat history show, e.g., role-based user/assistant messages).

#### Main Code: application.py (in app directory)
- Yeh main file hai jo Flask app run karta hai.
- **Imports**:
  - From Flask: `from flask import Flask, render_template, request, session, redirect, url_for` (class use karne ke liye bola, but standard Flask imports).
  - Retriever: `from app.components.retriever import create_qa_chain` (previous video se QA chain function – transcript mein "create QA chain" bola).
  - Env: `from dotenv import load_dotenv` (environment variables load ke liye).
  - OS: `import os`
  - Markup for safe rendering: `from markupsafe import Markup` (Jinja2 ke liye).

- **Load Environment Variables**:
  - `load_dotenv()` – .env file se sab variables load (e.g., HF_TOKEN) – yeh line sab env vars load karti hai.
  - Optional: `hf_token = os.environ.get('HF_TOKEN')` (Hugging Face token fetch, but ignore kar sakte ho, default se chalega).

- **Initialize Flask App**:
  - `app = Flask(__name__)` (name magic method `__name__` se).
  - Secret Key: `app.secret_key = os.urandom(24)` (random generate for sessions – 24 bytes).

- **Markup Filter for NL to BR (Optional but Recommended)**:
  - Function define: `def nl2br(value): return Markup(value.replace('\n', '<br>'))`
    - Explanation: LLM responses mein new lines (\n) inconvenient hote hain (e.g., "My name is Sid\nMy hobby is coding" ko <br> se proper lines mein convert – HTML <br> tag new line deta hai). Yeh attractive display ke liye (look wise better), warna results pe effect nahi. Example diya: Raw \n output ko formatted <br> mein change.
  - Register filter: `app.jinja_env.filters['nl2br'] = nl2br` (Jinja2 mein register – yeh three lines (\n to <br> conversion) karte hain safe HTML rendering ke liye).

- **Route 1: '/' (Home – GET/POST)**:
  - `@app.route('/', methods=['GET', 'POST'])`
  - Def: `def index():`
    - Session Init: `if 'messages' not in session: session['messages'] = []` (chat history ke liye empty list init if not present – messages include user queries aur assistant responses; yeh user queries/responses store karta hai).
    - Handle POST (user submits query):
      - `if request.method == 'POST':`
        - Fetch input: `user_input = request.form.get('prompt')` (index.html ke form se textarea name="prompt" se query le – side-by-side HTML open karke explain kiya).
        - If user_input: 
          - Messages list: `messages = session['messages']` (current session se le).
          - Append user message: `messages.append({'role': 'user', 'content': user_input})` (dictionary with role/content for history – chat history build karne ke liye, medical bot ko previous context mile).
          - Update session: `session['messages'] = messages`
        - Try block:
          - Create chain: `qa_chain = create_qa_chain()` (retriever se – transcript mein "create a chain sequel to create a chain" bola).
          - Invoke: `response = qa_chain.invoke(user_input)` (query pass karke response generate – user_input se result banao).
          - Extract result: `result = response.get('result', 'No response')` (response ek dictionary hai – keys like 'query' (user input), 'context' (vector store se), 'result' (final answer); sirf 'result' fetch karo, context/query ignore – yeh final LLM answer hai).
          - Append assistant message: `messages.append({'role': 'assistant', 'content': result})` (role 'assistant' ya kuch bhi like 'bot' – final answer store; user role 'user', assistant role 'assistant').
          - Update session: `session['messages'] = messages` (history update).
        - Except: `except Exception as e: error_message = str(e); return render_template('index.html', messages=session.get('messages', []), error=error_message)` (error handle aur display on page – Flask auto templates folder detect karta hai, isliye full path nahi).
      - Redirect: `return redirect(url_for('index'))` (POST ke baad refresh to index function).
    - Return for GET: `return render_template('index.html', messages=session.get('messages', []))` (chat history render – empty list if none).

- **Route 2: '/clear' (Clear Chat History)**:
  - `@app.route('/clear')`
  - Def: `def clear(): session.pop('messages', None); return redirect(url_for('index'))` (session se 'messages' remove/destroy – history delete, redirect to home/index function; yeh clear button ke liye, jaise ChatGPT mein conversation history clear).

- **Run the App**:
  - `if __name__ == '__main__': app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)`
    - Host 0.0.0.0 for production-grade.
    - Port 5000 (Flask default).
    - Debug False.
    - use_reloader=False (most important – sessions preserve karne ke liye, warna reload pe history delete ho jaati, context lost; Flask default reloader auto reload karta hai).

#### Demo and Running the App
- Terminal: Open new terminal/cmd, `python app/application.py` run karo – app starts.
- Logs: Loading vector store, embeddings (Hugging Face, SentenceTransformer init), chain create – URL like http://127.0.0.1:5000/ (logs se Ctrl+click to open).
- Test on Browser ("AI Medical Chatbot" page):
  - Query: "What is cancer?" → POST send, vector store check, output: "Cancer is a genetic disease with abnormal cells divide uncontrollably, disrupting the body's normal process." (correct).
  - Follow-up: "Thank you" → Hallucinates (e.g., "Thank you editors" – limited 10MB PDF data se, not trained for casual replies).
  - Another: "Can we cure cancer?" → "No not always" with therapies like chemotherapy/radiotherapy, but advanced stages weak chances.
- Issues:
  - Slow: Free model (Mistral), time lagta hai (paid like OpenAI/ChatGPT faster).
  - Hallucination: Non-medical/casual queries pe (e.g., "Thank you" pe irrelevant output), kyunki data limited.
  - Clear: Button click se history delete.
- Initial Error: Outdated packages – fixed (see below).

#### Fixes for Errors (LangChain Updates in lm.py)
- Problem: App nahi chali, kyunki LangChain changes (old version use kiya tha).
- Changes in lm.py (LLM setup):
  - Old: `from langchain.llms import HuggingFaceHub` aur `llm = HuggingFaceHub(repo_id=..., model_kwargs={temperature:0.3, max_length:256, return_full_text:False})` (deprecated).
  - New: `from langchain_huggingface import HuggingFaceEndpoint` (transcript mein "from long chain hugging face import hugging face endpoint").
    - `llm = HuggingFaceEndpoint(repo_id=HUGGINGFACE_REPO_ID, huggingfacehub_api_token=HF_TOKEN, temperature=0.3, max_new_tokens=256, return_full_text=False)` (dictionary nahi, separate params; max_length → max_new_tokens).
- Reason: HuggingFaceHub deprecated in current LangChain – endpoint use karo. Retriever aur baaki same.
- After Fix: Re-run, logs show embedding model init, chain success – app works.

#### Code Explanation Summary (End mein Repeat)
- Imports/Env: Basic setup, env load, token get.
- Markup NL to BR: Responses ko proper format ( \n → <br> – e.g., multi-line answers mein BR tags avoid garbled display).
- Session: Chat history maintain (list of dicts with 'role'/'content' – user/assistant). Yeh memory deta hai (conversation history build – e.g., follow-up queries previous context use, jaise ChatGPT mein "what I asked first?" pe recall).
  - Process: Session init → POST pe user input append (role 'user') → Chain invoke → Result append (role 'assistant') → Session update.
- Routes: Home for interaction (query process, render messages), Clear for reset (pop messages).
- Response: qa_chain.invoke se dictionary – sirf 'result' extract (query/context ignore).
- Rendering: index.html pe messages loop se display, error bhi show.
- Advice: Code confusing lage to copy-paste karo, line-by-line read – samajh aa jayega. index.html basic hai – ChatGPT se improve (interactive, animations), ya web dev experts apna banao (course mein web-to-AI switchers ke liye mention).

#### Video End
- Next video pe move.
- Explanation "not that good" bola, but hope clear hai – app running shown.

Bhai, yeh full detailed summary hai – code lines, explanations, demo, fixes sab include. Transcript same previous se, but yeh fresh hai with timestamps (34:57/35:01). Koi doubt ho toh bata!

