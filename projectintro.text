section 9.105

### Project Overview
The project is a Medical RAG (Retrieval-Augmented Generation) Chatbot. The speaker assumes the viewer has seen the demo video and knows the basic concept. It's built using a tech stack that's not basic but key components include Hugging Face, Jenkins, Aqua Trivy, Flask, and AWS Cloud. Jenkins and Aqua Trivy are highlighted as MVPs (Most Valuable Products/Tools) used in over 5000 companies and very famous.

### Tech Stack in Detail
- **Hugging Face**: Used for both the LLM (Large Language Model) and the embedding model. 
  - LLM: Mistral AI, an open-source model developed by a French startup. It's free; create an account, generate an access token, and use it to access the model. The LLM generates output based on input.
  - Embedding Model: Example is all-MiniLM-L6-v4 (or similar, referred to as mini L4). Converts documents into embeddings for storage in the vector store.
  - Data Source: A PDF file (medical encyclopedia). Extract content, convert to documents, then to embeddings using the embedding model.

- **Vector Store: FAISS**: A local vector store to store embeddings. Full form: Facebook AI Similarity Search. Developed by Meta (parent company of Facebook, Instagram; Facebook was renamed Meta in the past 2-3 years). Documents can't be stored directly; they must be converted to embeddings first.

- **LangChain**: A generative AI framework to interact with the models (LLM and embeddings).

- **PyPDF**: Library to read the contents of the PDF file, extract text, and convert it into documents. These documents are then turned into embeddings and stored in the vector store.

- **Flask and HTML/CSS**:
  - Flask: Handles backend operations. Processes user inputs (e.g., queries), performs logic, and returns responses.
  - HTML/CSS: Creates the UI (frontend). Users input data via forms (e.g., age in an example). Frontend sends requests to backend; backend processes and returns results to frontend for display.
  - Example: User enters age 18 in frontend form. Backend (Flask) checks if >=18; returns "eligible" or "not eligible" for driving, displayed on frontend. If age 16 or 15, shows "not eligible".

- **Docker**: Used for containerization during deployment. Create a Dockerfile to dockerize the application, allowing it to run as a container on AWS.

- **Aqua Trivy (referred to as Trivy or TV)**: An open-source security scanner. Scans Docker images for vulnerabilities (weaknesses or flaws in the application/Docker container that hackers can exploit for unauthorized access, data leakage, or misuse).
  - How it works: Detects issues like outdated libraries with security bugs (e.g., Flask version 2.0.1 has bugs; suggests upgrading to 2.1.0 where bugs are fixed). Scans requirements.txt, gives warnings and suggestions to preserve security from hackers.

- **Jenkins**: Used for CI/CD pipelines (Continuous Integration/Continuous Deployment). Automates: Code checkout from GitHub, building Docker image, pushing image, scanning with Trivy, and deploying to AWS Runner.
  - AWS Runner: A service by AWS to easily run applications on the cloud. Connect it to Jenkins; click "build" button to execute the entire pipeline.

- **GitHub**: Acts as SCM (Source Code Management). Stores all project code, tracks changes, and maintains records.

- **AWS Cloud**: For deployment. Includes ECR (Elastic Container Registry) to store Docker images (similar to Docker Hub or Google Container Registry). Images are pushed to ECR, then deployed to AWS Runner.

### Project Workflow
The workflow is detailed and sequential:

1. **Project and API Setup**: Create a virtual environment. Set up logging and custom exceptions (needed for production-grade projects). Define project structure with various folders and files. Create setup file, requirements file, and environment file to store API keys (e.g., Hugging Face access token).

2. **Hugging Face Setup**: Create/sign in to Hugging Face account, generate access token, store in environment variables. In configuration code, access environment variables, specify LLM version (Mistral AI) and embedding model.

3. **PDF Loader Code**: PDF stored in a data folder. Fetch PDF, read contents using PyPDF, convert to documents, then to chunks using LangChain.

4. **Embeddings Code**: Convert chunks into embeddings using the Hugging Face embedding model.

5. **Vector Store Code**: Store embeddings in FAISS vector store.

6. **Data Loader Code**: Combines the above: PDF loader + embeddings + vector store into one pipeline. Reads PDF, converts to chunks, to embeddings, and stores in vector store.

7. **LLM Setup Video**: Connect Hugging Face API to the specific LLM (Mistral AI). Define parameters: Prompt format, output format, temperature (0-1 scale for creativity; 0 = non-creative, 1 = highly creative; suggest 0.3-0.7 for balanced results. High temperature gives weird results; low gives minimal/no results).

8. **Retrieval Code**: For user queries, retrieve relevant data from the vector store (documents/embeddings). Send retrieved context to LLM for refined output. Creates a retriever to fetch and return answers.

9. **Main Application Code**: Combines frontend (HTML/CSS for UI/forms) and backend (Flask for logic/processing). As seen in demo video; handles user inputs and displays responses.

10. **Code Building Using GitHub**: Push all code to GitHub repository for storage and tracking.

11. **Dockerfile Creation**: Configures container creation: Install requirements, run setup files, specify Python image, define Flask port (5000), how to run the app (e.g., python app.py).

12. **Jenkins CI/CD Pipeline**:
    - Set up Jenkins on PC.
    - Integrate GitHub with Jenkins (code is in GitHub).
    - Pipeline steps: Checkout code from GitHub, build Docker image using Dockerfile, scan image with Trivy for vulnerabilities, push image to AWS ECR, deploy image to AWS Runner as an application.
    - All executes on clicking the "build" button in Jenkins.

The speaker hopes the viewer understands the main points, enjoys the project, learns something new, and can move forward.

9.106

### Project Setup for Medical RAG Chatbot

#### 1. Create Project Folder
- Navigate to your desired project directory.
- Create a new folder for the project.
- Name it "Medical rag chatbot" (or any preferred name, e.g., "rag medical chat bot").

#### 2. Open Folder in VS Code
- Open the folder in your terminal.
- Run the command `code .` to open the folder in VS Code.
- This will launch VS Code with the project folder as the workspace.

#### 3. Create and Activate Virtual Environment
- In VS Code, go to the top bar and select Terminal > New Terminal.
- Ensure the terminal is opened as Command Prompt.
- Run the command: `python -m venv venv` (you can use any name for the environment, but "venv" is used here).
- Wait 10-15 seconds for the virtual environment folder to be created (it will appear on the left side in VS Code).
- Activate the environment: Run `venv\Scripts\activate`.
- Confirmation: The terminal prompt will show `(venv)` at the beginning, indicating successful creation and activation.

#### 4. Create requirements.txt File
- In the root directory, create a file named `requirements.txt`.
- List the following libraries (one per line) for the project:
  - langchain (for core functionality).
  - langchain_community (additional community features).
  - langchain_huggingface (for Hugging Face integration).
  - faiss-cpu (for vector stores, used on CPU).
  - pypdf (for PDF parsing, to be used later).
  - huggingface_hub (for using Hugging Face API).
  - flask (for creating the web app).
  - python-dotenv (for handling environment variables, e.g., storing API keys).

#### 5. Set Up Project Structure
- In the root directory, create a folder named `app`.
- Inside `app`, create an `__init__.py` file (to treat `app` as a Python package).
- Inside `app`, create the following subfolders:
  - `common`: For common files like logging and custom exceptions.
    - Inside `common`, create `__init__.py` (to treat it as a package).
    - Create `logger.py` (for logging functionality).
    - Create `custom_exception.py` (for custom exceptions).
    - Copy the code for `logger.py` and `custom_exception.py` from GitHub (provided in resources or previous sections; the code has been explained thoroughly elsewhere).
  - `components`: For app components like LLM, retriever, PDF loader, data loader files, etc.
  - `config`: For configurations, such as loading APIs and models.
  - `templates`: For HTML web pages (required for Flask).

#### 6. Create setup.py File
- In the root directory, create a file named `setup.py`.
- Copy the code for `setup.py` from GitHub or previous resources (explained in detail elsewhere).
- Purpose: 
  - Collects and installs all packages listed in `requirements.txt`.
  - Handles project package management (e.g., treats folders with `__init__.py` as packages).
- Modify the project name in the code to "Rag medical Chatbot" (or your preferred name).
- Save the file.

#### 7. Install Requirements and Set Up Package
- Ensure the virtual environment is activated.
- In the terminal, run: `pip install -e .`.
- This triggers `setup.py`, installs all libraries from `requirements.txt`, and sets up the project as a package.
- Wait for installation to complete (time depends on internet speed and number of libraries).

#### 8. Prepare Data Folder and Download PDF
- In the root directory, create a folder named `data`.
- For the medical chatbot, download a medical PDF (e.g., an "encyclopedia of medicine" PDF, around 10 MB in size, from GitHub resources).
- You can use any relevant medical PDF.
- Copy the downloaded PDF from your downloads folder into the `data` directory.

#### 9. Set Up Hugging Face Account and API Token
- Open any browser and search for "Hugging Face".
- Create an account if you don't have one.
- Go to your profile section.
- Navigate to "Access Tokens".
- Generate a new token:
  - Select the "Write" section (options: fine-grained, read, write).
  - Name the token (any name).
  - Create the token; it will provide an access token key.
- Copy the token (create your own; do not use others as they may be deactivated).

#### 10. Create .env File for Environment Variables
- In the root directory, create a file named `.env`.
- Add the following lines:
  - `HF_TOKEN=your_huggingface_token` (paste the copied token here).
  - `HUGGINGFACE_API_TOKEN=your_huggingface_token` (paste the same token here; both are used for Hugging Face API access).
- Save the file.
- Note: This stores API keys securely for use via python-dotenv.

#### 11. Next Steps
- Wait for all library installations to complete successfully.
- Once done, proceed to the next video for further project development (e.g., PDF parsing, components implementation).

9.107

### Configuration Code Setup for Medical RAG Chatbot

#### 1. Update .env File
- Navigate to the `.env` file in the root directory.
- Change the entry from "HUGGINGFACE_API_TOKEN" to "HUGGINGFACE_HUB_API_TOKEN" (ensure the key is updated to match this exact name).
- Save the file.
- You can close the file after saving.

#### 2. Set Up config Directory as a Package
- Go to the `app` directory.
- Inside `app`, navigate to the `config` directory.
- Create a file named `__init__.py` inside `config` to treat it as a Python package.
- After creating `__init__.py`, open the terminal (ensure virtual environment is activated).
- Run the command: `pip install -e .` to update the packages, as a new `__init__.py` file has been added.
- Note: Every time you create a new `__init__.py` file or add a new library to the project, run `pip install -e .` to keep packages up to date.

#### 3. Create config.py File
- Inside the `config` directory, create a new file named `config.py`.
- This file will contain all project configurations.

#### 4. Write Configurations in config.py
- Import the `os` library at the top: `import os`.
- Fetch the Hugging Face token from the `.env` file:
  - `HF_TOKEN = os.environ.get('HF_TOKEN')` (use the exact name specified in your `.env` file, e.g., 'HF_TOKEN').
  - This stores the token in the `HF_TOKEN` variable.
- Define additional configuration variables:
  - Hugging Face repo ID (for the LLM model).
  - DB FAISS path (path for the vector store, to be created later).
  - Data path (path to input data, i.e., the PDF).
  - Chunk size: Set to 500 (related to LLM text processing).
  - Chunk overlap: Set to 50 (related to LLM text processing).
- Set the data path:
  - `DATA_PATH = 'data/'` (points to the `data` directory in the root where the PDF is stored; any files in `data` will be fetched as input).
- Set the DB FAISS path:
  - `DB_FAISS_PATH = 'vectorstore/db_faiss'` (this will create a `vectorstore` folder in the root directory, with a `db_faiss` subfolder inside to store all vector store data for the LLM).
- Set the Hugging Face repo ID:
  - Go to the Hugging Face website (huggingface.co).
  - Search for "models".
  - Search specifically for "Mistral".
  - Select "Mistral AI" and navigate to "Mistral-7B-Instruct-v0.3".
  - Copy the model path: `'mistralai/Mistral-7B-Instruct-v0.3'`.
  - Paste it as: `HUGGINGFACE_REPO_ID = 'mistralai/Mistral-7B-Instruct-v0.3'`.
  - Notes on the model (as per the video): It is free to use, open source, and Hugging Face provides about 100 requests per day, which is sufficient for testing purposes.
    - **Updated Note (as of August 2025)**: The model is an instruct fine-tuned version of Mistral-7B-v0.3 with extended vocabulary (32,768), v3 tokenizer support, and function calling. It lacks moderation mechanisms. It is open source (though license details like Apache 2.0 are typically applicable but not explicitly restated). Inference API is not directly deployed by providers, but accessible via Hugging Face tools; access requires login and sharing contact info. Free user rate limits for Hugging Face Inference API have changed: They are strict, often resetting hourly rather than daily, with recent reductions (e.g., from higher daily calls to more limited or paid tiers starting at ~$0.10 for certain usages; Pro plans offer higher limits like 20,000 requests/day but are not unlimited).

#### 5. Completion and Next Steps
- This completes all configurations in `config.py`.
- Proceed to the next video for further development.

9.108

### PDF Loader Code Setup for Medical RAG Chatbot

#### 1. Set Up components Directory as a Package
- Navigate to the `app` directory.
- Inside `app`, go to the `components` directory.
- Create a file named `__init__.py` inside `components` to treat it as a Python package.
- After creating `__init__.py`, run the command `pip install -e .` in the terminal (ensure virtual environment is activated) to update and treat the `components` directory as a package.
- Note: The installation may take time; once successful, you can close the terminal.

#### 2. Create pdf_loader.py File
- Inside the `components` directory, create a new file named `pdf_loader.py`.
- Purpose: This file handles loading PDF files from the data directory and splitting their content into text chunks for the RAG chatbot.
- Code can be copied from GitHub (mentioned in the resource section) if you don't want to write it from scratch, but for learning, follow the steps below.

#### 3. Imports in pdf_loader.py
- Import the following libraries and modules:
  - `import os` (for path handling).
  - From `langchain_community.document_loaders`:
    - `DirectoryLoader` (to load files from a directory).
    - `PyPDFLoader` (specifically for loading PDF files).
  - From `langchain.text_splitter`:
    - `RecursiveCharacterTextSplitter` (to split text into chunks; e.g., splits sentences like "My name is Dan" into smaller parts like "My", "name", "is", "Dan").
  - From `app.common.logger`:
    - `get_logger` (for logging functionality).
  - From `app.common.custom_exception`:
    - `CustomException` (for custom error handling).
  - From `app.config.config`:
    - `DATA_PATH` (path to the data directory containing PDFs).
    - `CHUNK_SIZE` (chunk size for text splitting, defined as 500 in config).
    - `CHUNK_OVERLAP` (chunk overlap for text splitting, defined as 50 in config).

#### 4. Initialize Logger
- Add: `logger = get_logger(__name__)` 
- This initializes the logger successfully using the module's name.

#### 5. Function: load_pdf_files()
- Purpose: Loads all PDF files from the `data` directory (handles any number of PDFs: 1, 3, 15, etc.; if multiple PDFs are added to `data`, it fetches all of them).
- Define the function: `def load_pdf_files():`
- Use a try-except block for error handling.
- Inside try:
  - Check if the data path exists: `if not os.path.exists(DATA_PATH):`
    - If not, raise `CustomException("Data path does not exist")`.
  - If it exists: `logger.info(f"Loading files from {DATA_PATH}")`.
  - Initialize the loader: `loader = DirectoryLoader(DATA_PATH, glob="*.pdf", loader_cls=PyPDFLoader)`
    - `DATA_PATH`: The directory to load from.
    - `glob="*.pdf"`: Specifies to fetch only PDF files (ignores text files, PowerPoint, etc.).
    - `loader_cls=PyPDFLoader`: Uses PyPDFLoader for PDF handling.
  - Load the documents: `documents = loader.load()`
    - All loaded PDF content is stored in `documents`.
  - Check if documents were loaded:
    - `if not documents: logger.warning("No PDFs were found")` (warning if no PDFs in data folder; suggests adding data).
    - Else: `logger.info(f"Successfully fetched {len(documents)} documents")` (logs the number of documents fetched, e.g., 1 PDF shows "1 document", 5 PDFs shows "5 documents").
  - Return the documents: `return documents`.
- Inside except: `except Exception as e:`
  - Create error message: `error_message = CustomException("Failed to load PDFs")`.
  - Raise the custom exception: `raise error_message`.
  - Log the error: `logger.error(str(error_message))`.
  - Optionally return an empty list: `return []` (this is optional and can be removed without affecting functionality; used if loading fails).

#### 6. Function: create_text_chunks(documents)
- Purpose: Takes the loaded documents, extracts their content, and splits it into smaller text chunks (e.g., breaking down text for LLM processing).
- Define the function: `def create_text_chunks(documents):`
- Use a try-except block for error handling.
- Inside try:
  - Check if documents exist: `if not documents:`
    - Raise `CustomException("No documents were found")`.
  - Log the splitting process: `logger.info(f"Splitting {len(documents)} documents into chunks")` (e.g., logs "Splitting 1 documents" or "Splitting 2 documents").
  - Initialize the text splitter: `text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)`
    - Uses the predefined `CHUNK_SIZE` (500) and `CHUNK_OVERLAP` (50) from config.
  - Split the documents: `text_chunks = text_splitter.split_documents(documents)`
    - Splits all document content into chunks, stored in `text_chunks`.
  - Log the result: `logger.info(f"Generated {len(text_chunks)} text chunks")` (e.g., shows how many chunks were created).
  - Return the chunks: `return text_chunks`.
- Inside except: `except Exception as e:`
  - Similar to the previous function: `error_message = CustomException("Failed to generate chunks")`.
  - Raise `error_message`.
  - `logger.error(str(error_message))`.
  - Optionally return an empty list: `return []`.

#### 7. Summary of pdf_loader.py
- Imports: Langchain document loaders (DirectoryLoader, PyPDFLoader), text splitter (RecursiveCharacterTextSplitter), logger, custom exception, and configurations (DATA_PATH, CHUNK_SIZE, CHUNK_OVERLAP).
- Initialize logger.
- Function `load_pdf_files()`: Loads all PDFs from the data directory, checks existence, logs details, handles multiple files, and returns documents (with exception handling).
- Function `create_text_chunks(documents)`: Splits loaded document content into chunks using the text splitter, logs the process, and returns the chunks (with exception handling).
- Overall: The file fetches PDF data and prepares it as chunked text for further use in the chatbot.

#### 8. Next Steps
- Proceed to the next video for further development.